#
#
#
# Authors: Michael Hoffman and Charlie Bonfield
# Created: May 2017

# Import statements
import operator
import numpy as np
import pandas as pd
import xgboost as xgb
import seaborn as sns
import matplotlib.pyplot as plt
#from boruta import BorutaPy as bp
#from sklearn import model_selection, preprocessing
#from sklearn.ensemble import RandomForestRegressor as rfr
#from statsmodels.stats.outliers_influence import variance_inflation_factor

#from astroML.plotting import setup_text_plots
#setup_text_plots(fontsize=12, usetex=True)

train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
macro_data = pd.read_csv('macro.csv')
train_latlon = pd.read_csv('train_lat_lon.csv')
test_latlon = pd.read_csv('test_lat_lon.csv')
train_latlon.drop(['key', 'tolerance_m'], axis=1, inplace=True)
test_latlon.drop(['key', 'tolerance_m'], axis=1, inplace=True)

# Add latitude, longitude estimates to training/test data.
train_data = pd.merge(train_data, train_latlon, on='id', how='left')
test_data = pd.merge(test_data, test_latlon, on='id', how='left')

"""
# Print shapes of training, test, and macro dataframes.
print('Training: ')
print(train_data.shape)
print('Test: ')
print(test_data.shape)
print('Macro: ')
print(macro_data.shape)

# Examine data
train_data.head()
"""

# Downsample by investment type in the training set, as there are differences
# between training/test in that regard (see more details at link below).
# URL: https://www.kaggle.com/c/sberbank-russian-housing-market/discussion/32717
train_sub = train_data[train_data.timestamp < '2015-01-01']
train_sub = train_sub[train_sub.product_type=="Investment"]

ind_1m = train_sub[train_sub.price_doc <= 1000000].index
ind_2m = train_sub[train_sub.price_doc == 2000000].index
ind_3m = train_sub[train_sub.price_doc == 3000000].index

train_index = set(train_data.index.copy())

for ind, gap in zip([ind_1m, ind_2m, ind_3m], [10, 3, 2]):
    ind_set = set(ind)
    ind_set_cut = ind.difference(set(ind[::gap]))

    train_index = train_index.difference(ind_set_cut)

train_data = train_data.loc[train_index]

# Split off columns that will be needed later.
train_ids = train_data['id'].values
test_ids = test_data['id'].values
train_prices = train_data['price_doc'].values * 0.969 + 10.0
train_lprices = np.log1p(train_prices)

train_data.drop(['id', 'price_doc'], axis=1, inplace=True)
test_data.drop(['id'], axis=1, inplace=True)

# Due to issues with multicollinearity, we want to only keep a subset of the
# features from the macro data. This list was generated by calculating/examining
# VIFs through an iterative process, documented below.
# URL: https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity
good_macro_features = ['timestamp','balance_trade', 'balance_trade_growth', 'eurrub',
                       'average_provision_of_build_contract', 'micex_rgbi_tr',
                       'micex_cbi_tr', 'deposits_rate', 'mortgage_value',
                       'mortgage_rate', 'income_per_cap', 'rent_price_4.room_bus',
                       'museum_visitis_per_100_cap', 'apartment_build']
good_macro_data = pd.DataFrame(macro_data, columns=good_macro_features)

# Remove weird rows prior to putting data together.
train_data = train_data[train_data.sub_area != 3]
train_data = train_data[train_data.product_type != 3]

# Merge good features from macro.csv to training/test data.
n_train = len(train_data.index)
all_tt_data = pd.concat([train_data, test_data], axis=0, ignore_index=True)
#all_data = pd.merge(all_tt_data, good_macro_data, on='timestamp', how='left')
all_data = all_tt_data.merge(good_macro_data, on='timestamp', how='left')
#print(all_data.shape)

# Fix a couple of values (based on another's work - cited below).
# https://www.kaggle.com/captcalculator/a-very-extensive-sberbank-exploratory-analysis
all_data.set_value(9484, 'state', 3)
all_data.set_value(9484, 'build_year', 2007)

### FEATURE ENGINEERING
## I. Extracting Features from Timestamps
print('Extracting features from timestamps. . .')

# Extract year, month, day of week, and week of year.
# Taken from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
years = pd.to_datetime(all_data.timestamp, errors='coerce').dt.year
months = pd.to_datetime(all_data.timestamp, errors='coerce').dt.month
dows = pd.to_datetime(all_data.timestamp, errors='coerce').dt.dayofweek
woys = pd.to_datetime(all_data.timestamp, errors='coerce').dt.weekofyear
doys = pd.to_datetime(all_data.timestamp, errors='coerce').dt.dayofyear

# Extract number of sales in month/year combos, week/year combos.
# Taken from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
month_year = (months + years * 100)
month_year_cnt_map = month_year.value_counts().to_dict()
week_year = (woys + years * 100)
week_year_cnt_map = week_year.value_counts().to_dict()

# Add all new features to existing data frame.
all_data['year'] = years
all_data['month'] = months
#all_data['day_of_week'] = dows      # do not see fluctuations on this scale (see below).
#all_data['day_of_year'] = doys      # do not see fluctuations on this scale (see below).
all_data['month_year_count'] = month_year.map(month_year_cnt_map)
all_data['week_year_count'] = week_year.map(week_year_cnt_map)

# Drop timestamps.
all_data.drop(['timestamp'], axis=1, inplace=True)

## II. Property-Specific Features
# Fix poor and/or likely incorrect property data.
c0 = all_data[all_data.floor == 0.0].index
all_data.ix[c0, 'floor'] = np.nan
c1 = all_data[all_data.max_floor < all_data.floor].index
all_data.ix[c1, 'max_floor'] = np.nan
c2 = all_data[all_data.full_sq < all_data.life_sq].index
all_data.ix[c2, 'life_sq'] = np.nan
c3 = all_data[all_data.kitch_sq == 0.0].index
all_data.ix[c3, 'kitch_sq'] = np.nan
c4 = all_data[all_data.kitch_sq == 1.0].index
all_data.ix[c4, 'kitch_sq'] = np.nan
c5 = all_data[all_data.life_sq < 5.0].index
all_data.ix[c5, 'life_sq'] = np.nan
c6 = all_data[all_data.full_sq < 5.0].index
all_data.ix[c6, 'full_sq'] = np.nan

# Feature engineering.
all_data['max_floor'] = all_data['max_floor'].replace(to_replace=0, value=np.nan)
all_data['rel_floor'] = all_data['floor'] / all_data['max_floor'].astype(float)
all_data['rel_kitch_sq'] = all_data['kitch_sq'] / all_data['full_sq'].astype(float)
all_data['rel_life_sq'] = all_data['life_sq'] / all_data['full_sq'].astype(float)
all_data['rel_life_sq'] = all_data['rel_life_sq'].replace(to_replace=np.inf, value=np.nan) # Corrects for property with zero full_sq.
all_data['avg_room_sq'] = all_data['life_sq'] / all_data['num_room'].astype(float) # Does not account for living room, but reasonable enough.
all_data['avg_room_sq'] = all_data['avg_room_sq'].replace(to_replace=np.inf, value=np.nan) # Corrects for studios (zero rooms listed).

# Replace garbage values in build_year with NaNs, then find average build year
# in each sub_area.
all_data['build_year'] = all_data['build_year'].replace(to_replace=[0,1,2,3,20,71,215,4965], value=np.nan)
mean_by_districts = pd.DataFrame(columns=['district', 'avg_build_year'])
sub_areas_unique = all_data['sub_area'].unique()
for sa in sub_areas_unique:
    temp = all_data.loc[all_data['sub_area'] == sa]
    mean_build_year = temp['build_year'].mean()
    new_df = pd.DataFrame([[sa, mean_build_year]], columns=['district', 'avg_build_year'])
    mean_by_districts = mean_by_districts.append(new_df, ignore_index=True)

mbd_dis_list = mean_by_districts['district'].tolist()
mbd_dis_full = all_data['sub_area'].tolist()
mbd_aby_np = np.array(mean_by_districts['avg_build_year'])
mbd_aby_full = np.zeros(len(all_data.index))

# (Could find a better way to do this.)
for i in range(len(all_data.index)):
    district = mbd_dis_full[i]
    mbd_aby_full[i] = mbd_aby_np[mbd_dis_list.index(district)]

all_data['avg_build_year'] = mbd_aby_full
all_data['rel_build_year'] = all_data['build_year'] - all_data['avg_build_year']

## III. Categorical Features, Treating NaNs

# Deal with categorical values.
# Adapted from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
df_numeric = all_data.select_dtypes(exclude=['object'])
df_obj = all_data.select_dtypes(include=['object']).copy()
ecology_dict = {'no data': np.nan, 'poor': 1, 'satisfactory': 2, 'good': 3,
                'excellent': 4}
no_yes_dict = {'no': 0, 'yes': 1}

def one_hot_encode(x, n_classes):
    #One hot encode a list of sample labels. Return a one-hot encoded vector for each label.
    #: x: List of sample Labels
    #: return: Numpy array of one-hot encoded labels
    return np.eye(n_classes)[x]

for c in df_obj:
    factorized = pd.factorize(df_obj[c])
    f_values = factorized[0]
    f_labels = list(factorized[1])

    if len(f_labels) == 3:
        print(c)
        print(f_labels)

    n_classes = len(f_labels)

    if (n_classes == 2 or n_classes == 3) and c != 'product_type':
        df_obj[c] = factorized[0]
    elif c == 'ecology':
        df_obj[c] = df_obj[c].map(ecology_dict)
    else:
        #df_obj[c] = pd.factorize(df_obj[c])[0]
        one_hot_features = one_hot_encode(f_values, n_classes)
        oh_features_df = pd.DataFrame(one_hot_features, columns=f_labels)
        df_obj = df_obj.drop(c, axis=1)
        df_obj = pd.concat([df_obj, oh_features_df], axis=1)

all_values = pd.concat([df_numeric, df_obj], axis=1)

# Fill all NaNs with -9999 (hacky fix that should allow Boruta to do it's thing).
#all_values = all_values.fillna(value=-9999)
full_feature_names = list(all_values)



### XGBOOST (for prediction)
# After feature engineering has been completed, split data back into training/test sets.
# Taken (with modifications) from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
print('Splitting data back into training/test sets. . .')
x_all = all_values.values

# Create a validation set with last 20% of training data.
n_val = int(n_train * 0.2)

x_train_all = x_all[:n_train]
x_train_all_df = pd.DataFrame(x_train_all, columns=full_feature_names)
x_train_sub = x_all[:n_train-n_val]
x_train_val = x_all[n_train-n_val:n_train]
y_train_all = train_lprices.ravel()      # Log(price)
y_train_sub = y_train_all[:-n_val]       # Log(price)
y_train_val = y_train_all[-n_val:]       # Log(price)

x_test = x_all[n_train:]
x_test_df = pd.DataFrame(x_test, columns=full_feature_names)

lprices_df = pd.Series(train_lprices, name='log_price')

# Copied from same notebook as some of code above.
dtrain_all = xgb.DMatrix(x_train_all, y_train_all, feature_names=full_feature_names)
dtrain = xgb.DMatrix(x_train_sub, y_train_sub, feature_names=full_feature_names)
dval = xgb.DMatrix(x_train_val, y_train_val, feature_names=full_feature_names)
dtest = xgb.DMatrix(x_test, feature_names=full_feature_names)

xgb_params = {
    'eta': 0.05,
    'max_depth': 4,
    'subsample': 0.8,
    'colsample_bytree': 0.65,
    'objective': 'reg:linear',
    'eval_metric': 'rmse',
    'min_child_weight': 4,
    'silent': 1,
    'seed':0
}

# Train model using tuned hyperparameters (found with xgb.cv).
#num_boost_round = 416

# Perform cross-validation on training set.
cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,
                   verbose_eval=50, show_stdv=False)
num_boost_round = len(cv_output)
print(num_boost_round)

model = xgb.train(dict(xgb_params, silent=0), dtrain_all, num_boost_round=num_boost_round)


# Plot feature importance.
fig, ax = plt.subplots(1, 1, figsize=(8, 16))
xgb.plot_importance(model, height=0.2, ax=ax)


# Predict log(price) + 1 for test set.
ylog_pred = model.predict(dtest)
y_pred = np.expm1(ylog_pred)

df_sub = pd.DataFrame({'id': test_ids, 'price_doc': y_pred})

df_sub.to_csv('sub_prices_cv_ohe.csv', index=False)
