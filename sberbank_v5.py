#
#
#
# Author: Charlie Bonfield
# Created: May 2017

# Import statements 
import numpy as np 
import pandas as pd
import xgboost as xgb
import seaborn as sns
import matplotlib.pyplot as plt
#from boruta import BorutaPy as bp
#from sklearn import model_selection, preprocessing
#from sklearn.ensemble import RandomForestRegressor as rfr
#from statsmodels.stats.outliers_influence import variance_inflation_factor

#from astroML.plotting import setup_text_plots
#setup_text_plots(fontsize=12, usetex=True)

train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
macro_data = pd.read_csv('macro.csv')

"""
# Print shapes of training, test, and macro dataframes.
print('Training: ') 
print(train_data.shape)
print('Test: ')
print(test_data.shape)
print('Macro: ')
print(macro_data.shape)

# Examine data
train_data.head()
"""

# Split off columns that will be needed later. 
train_ids = train_data['id'].values
test_ids = test_data['id'].values
train_prices = train_data['price_doc'].values
train_lprices = np.log1p(train_prices)

train_data.drop(['id', 'price_doc'], axis=1, inplace=True)
test_data.drop(['id'], axis=1, inplace=True)

# Due to issues with multicollinearity, we want to only keep a subset of the 
# features from the macro data. This list was generated by calculating/examining
# VIFs through an iterative process, documented below. 
# URL: https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity
good_macro_features = ['timestamp','balance_trade', 'balance_trade_growth', 'eurrub', 
                       'average_provision_of_build_contract', 'micex_rgbi_tr',
                       'micex_cbi_tr', 'deposits_rate', 'mortgage_value', 
                       'mortgage_rate', 'income_per_cap', 'rent_price_4.room_bus',
                       'museum_visitis_per_100_cap', 'apartment_build']
good_macro_data = pd.DataFrame(macro_data, columns=good_macro_features)

# Merge good features from macro.csv to training/test data. 
n_train = len(train_data.index)
all_tt_data = pd.concat([train_data, test_data])
all_data = pd.merge_ordered(all_tt_data, good_macro_data, on='timestamp', how='left')
#print(all_data.shape)

# Fix a couple of values (based on another's work - cited below). 
# https://www.kaggle.com/captcalculator/a-very-extensive-sberbank-exploratory-analysis
all_data.loc[all_data.state == 33] = 3
all_data.loc[all_data.build_year == 20052009] = 2007

### FEATURE ENGINEERING
## I. Extracting Features from Timestamps
print('Extracting features from timestamps. . .')

# Extract year, month, day of week, and week of year.
# Taken from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
years = pd.to_datetime(all_data.timestamp, errors='coerce').dt.year
months = pd.to_datetime(all_data.timestamp, errors='coerce').dt.month
dows = pd.to_datetime(all_data.timestamp, errors='coerce').dt.dayofweek
woys = pd.to_datetime(all_data.timestamp, errors='coerce').dt.weekofyear
doys = pd.to_datetime(all_data.timestamp, errors='coerce').dt.dayofyear

# Extract number of sales in month/year combos, week/year combos. 
# Taken from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
month_year = (months + years * 100)
month_year_cnt_map = month_year.value_counts().to_dict()
week_year = (woys + years * 100)
week_year_cnt_map = week_year.value_counts().to_dict()

# Add all new features to existing data frame. 
all_data['year'] = years
all_data['month'] = months
#all_data['day_of_week'] = dows      # do not see fluctuations on this scale (see below).
#all_data['day_of_year'] = doys      # do not see fluctuations on this scale (see below).
all_data['month_year_count'] = month_year.map(month_year_cnt_map)
all_data['week_year_count'] = week_year.map(week_year_cnt_map)

# Drop timestamps. 
all_data.drop(['timestamp'], axis=1, inplace=True)

## II. Property-Specific Features
all_data['max_floor'] = all_data['max_floor'].replace(to_replace=0, value=np.nan)
all_data['rel_floor'] = all_data['floor'] / all_data['max_floor'].astype(float)
all_data['rel_kitch_sq'] = all_data['kitch_sq'] / all_data['full_sq'].astype(float)
all_data['rel_life_sq'] = all_data['life_sq'] / all_data['full_sq'].astype(float)
all_data['rel_life_sq'] = all_data['rel_life_sq'].replace(to_replace=np.inf, value=np.nan) # Corrects for property with zero full_sq. 
all_data['avg_room_sq'] = all_data['life_sq'] / all_data['num_room'].astype(float) # Does not account for living room, but reasonable enough.
all_data['avg_room_sq'] = all_data['avg_room_sq'].replace(to_replace=np.inf, value=np.nan) # Corrects for studios (zero rooms listed). 

# Replace garbage values in build_year with NaNs, then find average build year
# in each sub_area. 
all_data['build_year'] = all_data['build_year'].replace(to_replace=[0,1,2,3,20,71,215,4965], value=np.nan)
mean_by_districts = pd.DataFrame(columns=['district', 'avg_build_year'])
sub_areas_unique = all_data['sub_area'].unique()
for sa in sub_areas_unique:
    temp = all_data.loc[all_data['sub_area'] == sa]
    mean_build_year = temp['build_year'].mean()
    new_df = pd.DataFrame([[sa, mean_build_year]], columns=['district', 'avg_build_year'])
    mean_by_districts = mean_by_districts.append(new_df, ignore_index=True)

mbd_dis_list = mean_by_districts['district'].tolist()
mbd_dis_full = all_data['sub_area'].tolist()
mbd_aby_np = np.array(mean_by_districts['avg_build_year'])
mbd_aby_full = np.zeros(len(all_data.index))

# (Could find a better way to do this.)
for i in range(len(all_data.index)):
    district = mbd_dis_full[i]
    mbd_aby_full[i] = mbd_aby_np[mbd_dis_list.index(district)]
    
all_data['avg_build_year'] = mbd_aby_full
all_data['rel_build_year'] = all_data['build_year'] - all_data['avg_build_year']

## III. Categorical Features, Treating NaNs

# Deal with categorical values. 
# Taken from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
df_numeric = all_data.select_dtypes(exclude=['object'])
df_obj = all_data.select_dtypes(include=['object']).copy()

for c in df_obj:
    df_obj[c] = pd.factorize(df_obj[c])[0]

all_values = pd.concat([df_numeric, df_obj], axis=1)

# Fill all NaNs with -9999 (hacky fix that should allow Boruta to do it's thing).
#all_values = all_values.fillna(value=-9999)
full_feature_names = list(all_values)

"""
# Apply mask to features, keeping only the ones selected externally by Boruta.
feature_mask = np.loadtxt('feature_mask.txt', dtype=int)
boolean_mask = []

for feature in feature_mask:
    if ((feature == 1) or (feature == 2)):
        boolean_mask.append(True)
    else:
        boolean_mask.append(False)
        
boruta_features = all_values.columns[boolean_mask]
all_values_boruta = all_values[boruta_features]
"""

# After feature engineering has been completed, split data back into training/test sets. 
# Taken (with modifications) from: https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317
print('Splitting data back into training/test sets. . .')
x_all = all_values.values

# Create a validation set with last 20% of training data.
n_val = int(n_train * 0.2)

x_train_all = x_all[:n_train]
x_train_all_df = pd.DataFrame(x_train_all, columns=full_feature_names)
x_train_sub = x_all[:n_train-n_val]
x_train_val = x_all[n_train-n_val:n_train]
y_train_all = train_lprices.ravel()      # Log(price)
y_train_sub = y_train_all[:-n_val]       # Log(price)
y_train_val = y_train_all[-n_val:]       # Log(price)

x_test = x_all[n_train:]
x_test_df = pd.DataFrame(x_test, columns=full_feature_names)

lprices_df = pd.Series(train_lprices, name='log_price')
"""
train_all = pd.concat([all_values.ix[0:(n_train-1)],lprices_df], axis=1)
train_all.to_csv('train_boruta.csv')
"""

"""
# Check to make sure that everything is the correct size.
print('Training Data (x_all): ', x_train_all.shape)
print('Training Data (x_sub): ', x_train_sub.shape)
print('Training Data (y_sub): ', y_train_sub.shape)
print('Training Data (x_val): ', x_train_val.shape)
print('Training Data (y_val): ', y_train_val.shape)
print('Test Data (x): ', x_test.shape)
"""
"""
# Check for multicollinearity in the macro data. 
# NOTE: I have made use of the results from the link below. If you wish to 
# test for multicollinearity indepedently, uncomment this block of code and 
# comment the block at the top of the page where we select only the 13 
# listed. 
# URL: https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity
print('Checking for multicollinearity in the macro data. . .')
macro_data_plus_ones = macro_data
macro_data_plus_ones['dummy'] = np.ones(len(macro_data.index))
#macro_data_plus_ones = macro_data_plus_ones.fillna(value=-99)
#macro_data_plus_ones.drop(['timestamp'], axis=1, inplace=True)
#mdpo_numeric = macro_data_plus_ones.select_dtypes(exclude=['object'])
#mdpo_obj = macro_data_plus_ones.select_dtypes(include=['object']).copy()
#for c in mdpo_obj:
#    mdpo_obj[c] = pd.factorize(mdpo_obj[c])[0]
#mdpo_df = pd.concat([mdpo_numeric, mdpo_obj], axis=1)
#mdpo_np = np.array(mdpo_df)

#vif = [variance_inflation_factor(mdpo_np, i) for i in range(mdpo_np.shape[1])]
#print(vif)
"""
"""
### FEATURE SELECTION (BorutaPy)

# Define random forest regressor, utilising single core and sampling in proportion to y labels.
# To utilize all cores, set n_jobs = -1. 
print('Building random forest regressor. . .')
random_forest = rfr(n_jobs=-1, criterion='mse', max_depth=None, max_features=None)

# Define the Boruta feature selection method. 
print('Defining feature selection method. . .')
feature_selector = bp(random_forest, n_estimators='auto', verbose=2, random_state=1)

# Find all relevant features. 
print('Finding relevant features. . .')
feature_selector.fit(x_train_all, y_train_all)

# Check selected features.
print('Checking selected features. . .')
feature_selector.support_

# Check ranking of features.
print('Displaying ranking of features. . .')
feature_selector.ranking_

# call transform() on X to filter it down to selected features
#x_train_all_filtered = feature_selector.transform(x_train_all)
"""
### XGBOOST (for prediction)
# Copied from same notebook as some of code above. 
dtrain_all = xgb.DMatrix(x_train_all, y_train_all, feature_names=full_feature_names)
dtrain = xgb.DMatrix(x_train_sub, y_train_sub, feature_names=full_feature_names)
dval = xgb.DMatrix(x_train_val, y_train_val, feature_names=full_feature_names)
dtest = xgb.DMatrix(x_test, feature_names=full_feature_names)

# Specify grid of hyperparameters that we wish to iterate over. 
etas = np.linspace(0.05, 0.3, 3)
mds = np.arange(3,8,1)
subsamples = np.linspace(0.5, 0.8, 3)
colsamples = np.linspace(0.5, 0.8, 3)
min_childs = np.arange(1,5,1)

columns=['eta', 'max_depth', 'subsample', 'colsample_bytree', 'min_child_weight',
         'num_boost_round', 'rmse']

hyperparameter_tuning = pd.DataFrame(columns=columns)
i=0

for eta in etas:
    for md in mds:
        for sub in subsamples:
            for col in colsamples:
                for min_child in min_childs:
                    xgb_params = {
                        'eta': eta,
                        'max_depth': md,
                        'subsample': sub,
                        'colsample_bytree': col,
                        'objective': 'reg:linear',
                        'eval_metric': 'rmse',
                        'min_child_weight': min_child,
                        'silent': 1,
                        'seed': 0
                    }
                    print('Iteration: %f' % i)
                    
                    # Tune XGB num_boost_round. 
                    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],
                                              early_stopping_rounds=20, verbose_eval=20)

                    num_boost_round = partial_model.best_iteration
                    rmse_best = partial_model.best_score
                    
                    single_iteration = pd.DataFrame([[eta, md, sub, col, min_child, 
                                                      num_boost_round, rmse_best]], columns=columns)
                    hyperparameter_tuning = hyperparameter_tuning.append(single_iteration) 
                    
                    i += 1

hyperparameter_tuning.to_csv('hp_tuning.csv')
"""
# Plot feature importance.
fig, ax = plt.subplots(1, 1, figsize=(8, 16))
xgb.plot_importance(partial_model, height=0.2, ax=ax)
"""
"""
# Train model using CV num_boost_round.
model = xgb.train(dict(xgb_params, silent=0), dtrain_all, num_boost_round=num_boost_round)

# Predict log(price) + 1 for test set. 
ylog_pred = model.predict(dtest)
y_pred = np.expm1(ylog_pred)

df_sub = pd.DataFrame({'id': test_ids, 'price_doc': y_pred})

df_sub.to_csv('sub.csv', index=False)
"""